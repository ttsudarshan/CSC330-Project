{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# DataFrame Import\n",
    "Xander Palermo"
   ],
   "id": "d701e08ede997043"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This file extracts an existing the f1db from the Postgres dump provided by the F1DB project. This file then saves a handful of pickled DataFrames to be used in other notebooks",
   "id": "774514fc534bac8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:41:06.328766Z",
     "start_time": "2025-12-01T19:41:06.308354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import sqlalchemy\n",
    "import pandas as pd"
   ],
   "id": "2cfb234d8a32ad1",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Give access to notebook for Postgres Database actions",
   "id": "b6df6e21033b75b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:41:10.398744Z",
     "start_time": "2025-12-01T19:41:06.342988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Replace these with your actual credentials\n",
    "db_username = \"postgres\"\n",
    "db_password = input(\"Database Password: \")\n",
    "db_host = \"localhost\"\n",
    "db_port = \"5432\"\n",
    "db_name = \"f1db\"\n"
   ],
   "id": "300644f8b5d4a496",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:41:10.485920Z",
     "start_time": "2025-12-01T19:41:10.474478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to postgres database\n",
    "engine = sqlalchemy.create_engine(f\"postgresql+psycopg2://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}\")"
   ],
   "id": "2a518da431620334",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import Info\n",
    "Functionality of these tools are saved to *DataFrameImport.py* so that they can be utilized in other Notebooks.\n",
    "\n",
    "``from DataFrameImport import *``"
   ],
   "id": "b5bb82752e238ddc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Table Look Up Functionality\n",
    "Create a function that can be used to look up specific tables, or list all table"
   ],
   "id": "190b392de9f0c20e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:41:10.499566Z",
     "start_time": "2025-12-01T19:41:10.493584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "schema_info_path = \"resources/f1db/\"\n",
    "schema_file = \"f1db.schema.json\"\n",
    "\n",
    "with open(schema_info_path + schema_file, 'r') as f:\n",
    "    info_json = json.load(f)\n",
    "\n",
    "schema_names = list(dict(info_json[\"properties\"].items()).keys())\n",
    "schema_descriptions = [x[\"description\"] for x in list(dict(info_json[\"properties\"].items()).values())]\n",
    "\n",
    "schema_info = dict(zip(schema_names, schema_descriptions))\n",
    "\n",
    "with open(\"resources/info/schema_info_dump.pkl\", \"wb\") as file:\n",
    "    pickle.dump(schema_info, file)\n",
    "\n",
    "# Prints alias of every table\n",
    "def list_schemas():\n",
    "    for s in schema_info.keys():\n",
    "        print(s)\n",
    "\n",
    "#Prints table alias and description, or provide specific table alias to get single description\n",
    "def get_schema_info(t: str = \"\"):\n",
    "    if t in schema_info.keys():\n",
    "        print(t)\n",
    "        print(schema_info[t])\n",
    "        print()\n",
    "    else:\n",
    "        for t in schema_info.keys():\n",
    "            get_schema_info(t)\n",
    "\n",
    "# ex:\n",
    "# list_schemas()\n",
    "# get_schema_info()"
   ],
   "id": "6f6cce525104148",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Column Look Up Functionality\n",
    "Create an object that holds information of all schema descriptions and data types (accessed using dot notation)\n"
   ],
   "id": "11abcb433cf3e287"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:41:10.505174Z",
     "start_time": "2025-12-01T19:41:10.503283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Placeholder class that is used to attach dynamically named attributes\n",
    "class Null:\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(f\"{k} = {v}\" for k, v in vars(self).items()) + \"\\n\""
   ],
   "id": "49727a9cf8470ead",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:41:10.730527Z",
     "start_time": "2025-12-01T19:41:10.510352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "column_info = info_json['definitions']\n",
    "\n",
    "with engine.connect() as c:\n",
    "    query = c.execute(sqlalchemy.text(\"\"\n",
    "                                      \"SELECT table_name \"\n",
    "                                      \"FROM information_schema.tables \"\n",
    "                                      \"WHERE table_schema = 'public' \"\n",
    "                                      \"    AND table_type = 'BASE TABLE';\"))\n",
    "    table_names = query.all()\n",
    "\n",
    "table_names = [list(i)[0] for i in table_names]\n",
    "\n",
    "table_var_names = list(map(lambda x: x.replace('_', ' ').title().replace(' ', ''), table_names))\n",
    "\n",
    "schema = Null()\n",
    "\n",
    "for table in table_var_names:\n",
    "    try:\n",
    "        #Create column object\n",
    "        attr_names = list(column_info[table]['properties'].keys())\n",
    "        exec(f\"schema.{table} = Null()\")\n",
    "\n",
    "\n",
    "        for attr_name in attr_names:\n",
    "            # Assign type and description name\n",
    "            exec(f\"schema.{table}.{attr_name} = Null()\")\n",
    "\n",
    "            try:\n",
    "                attr_type = column_info[table]['properties'][attr_name][\"type\"]\n",
    "                exec(f\"schema.{table}.{attr_name}.type = attr_type\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                attr_desc = column_info[table]['properties'][attr_name][\"description\"]\n",
    "                exec(f\"schema.{table}.{attr_name}.description = attr_desc\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "with open(\"resources/info/schema_columns_info_dump.pkl\", \"wb\") as file:\n",
    "    pickle.dump(schema, file)\n",
    "\n",
    "\n",
    "# ex:\n",
    "# schema.{table_name}.{column_name(optional)}\n",
    "# print(schema.Continent)           #Gives info on all columns in DataFame Continent\n",
    "# print(schema.Continent.id)        #Gives info on specific column (id) in DataFrame Continent"
   ],
   "id": "5b9901f73512ddc1",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pickle Tables\n",
    "Saving tables as pickled DataFrames so that work can be independently from existence of a postgres database."
   ],
   "id": "5ee10b3077ddb63f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:41:13.162262Z",
     "start_time": "2025-12-01T19:41:10.736432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"resources/pickled_tables/\"\n",
    "extension = \".plk\"\n",
    "\n",
    "for table in table_names:\n",
    "    with engine.connect() as conn:\n",
    "        query = f\"SELECT * FROM {table}\"\n",
    "\n",
    "        dataFrame = pd.read_sql_query(query, conn)\n",
    "        dataFrame.to_pickle(f'{path}{table}{extension}')\n",
    "\n",
    "# Read using the following code\n",
    "# table = dataFrame you want to load\n",
    "# with open(f\"resources/pickled_tables/{table}.pkl\",\"rb\") as file:\n",
    "#   dataFrame = pickle.load(file)"
   ],
   "id": "4e700e82beae60f4",
   "outputs": [],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
